{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 반도체 박막 두께 알고리즘 분석 대회 By Dacon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#독립변수와 종속변수를 분리합니다.\n",
    "train_X = train.iloc[:,4:]  # 독립변수 -> 반사율(0~225)\n",
    "train_Y = train.iloc[:,0:4] # 종족변수 -> layer 1~4 두께\n",
    "test_X = test.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.254551</td>\n",
       "      <td>0.258823</td>\n",
       "      <td>0.254659</td>\n",
       "      <td>0.252085</td>\n",
       "      <td>0.247678</td>\n",
       "      <td>0.253614</td>\n",
       "      <td>0.246511</td>\n",
       "      <td>0.259407</td>\n",
       "      <td>0.260862</td>\n",
       "      <td>0.242524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.354750</td>\n",
       "      <td>0.369223</td>\n",
       "      <td>0.388184</td>\n",
       "      <td>0.408496</td>\n",
       "      <td>0.414564</td>\n",
       "      <td>0.429403</td>\n",
       "      <td>0.419225</td>\n",
       "      <td>0.443250</td>\n",
       "      <td>0.433414</td>\n",
       "      <td>0.465502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.205062</td>\n",
       "      <td>0.225544</td>\n",
       "      <td>0.217758</td>\n",
       "      <td>0.202169</td>\n",
       "      <td>0.199633</td>\n",
       "      <td>0.207380</td>\n",
       "      <td>0.191318</td>\n",
       "      <td>0.195369</td>\n",
       "      <td>0.200536</td>\n",
       "      <td>0.197588</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557203</td>\n",
       "      <td>0.573656</td>\n",
       "      <td>0.587998</td>\n",
       "      <td>0.612754</td>\n",
       "      <td>0.627825</td>\n",
       "      <td>0.633393</td>\n",
       "      <td>0.637706</td>\n",
       "      <td>0.625981</td>\n",
       "      <td>0.653231</td>\n",
       "      <td>0.637853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.189196</td>\n",
       "      <td>0.165869</td>\n",
       "      <td>0.177655</td>\n",
       "      <td>0.156822</td>\n",
       "      <td>0.175094</td>\n",
       "      <td>0.177755</td>\n",
       "      <td>0.157582</td>\n",
       "      <td>0.158885</td>\n",
       "      <td>0.156911</td>\n",
       "      <td>0.166162</td>\n",
       "      <td>...</td>\n",
       "      <td>0.699864</td>\n",
       "      <td>0.708688</td>\n",
       "      <td>0.721982</td>\n",
       "      <td>0.713464</td>\n",
       "      <td>0.743030</td>\n",
       "      <td>0.741709</td>\n",
       "      <td>0.747743</td>\n",
       "      <td>0.746037</td>\n",
       "      <td>0.737356</td>\n",
       "      <td>0.750391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.131003</td>\n",
       "      <td>0.120076</td>\n",
       "      <td>0.138975</td>\n",
       "      <td>0.117931</td>\n",
       "      <td>0.130566</td>\n",
       "      <td>0.131262</td>\n",
       "      <td>0.126962</td>\n",
       "      <td>0.134453</td>\n",
       "      <td>0.106717</td>\n",
       "      <td>0.127309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.764786</td>\n",
       "      <td>0.763788</td>\n",
       "      <td>0.770017</td>\n",
       "      <td>0.787571</td>\n",
       "      <td>0.778866</td>\n",
       "      <td>0.776969</td>\n",
       "      <td>0.774712</td>\n",
       "      <td>0.801526</td>\n",
       "      <td>0.805305</td>\n",
       "      <td>0.784057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.091033</td>\n",
       "      <td>0.086893</td>\n",
       "      <td>0.108125</td>\n",
       "      <td>0.080405</td>\n",
       "      <td>0.105917</td>\n",
       "      <td>0.077083</td>\n",
       "      <td>0.097895</td>\n",
       "      <td>0.086765</td>\n",
       "      <td>0.078676</td>\n",
       "      <td>0.075729</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786677</td>\n",
       "      <td>0.802271</td>\n",
       "      <td>0.806557</td>\n",
       "      <td>0.799614</td>\n",
       "      <td>0.789333</td>\n",
       "      <td>0.804087</td>\n",
       "      <td>0.787763</td>\n",
       "      <td>0.794948</td>\n",
       "      <td>0.819105</td>\n",
       "      <td>0.801781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809995</th>\n",
       "      <td>0.349513</td>\n",
       "      <td>0.342661</td>\n",
       "      <td>0.326351</td>\n",
       "      <td>0.274503</td>\n",
       "      <td>0.253076</td>\n",
       "      <td>0.200509</td>\n",
       "      <td>0.159043</td>\n",
       "      <td>0.094644</td>\n",
       "      <td>0.074915</td>\n",
       "      <td>0.054888</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534272</td>\n",
       "      <td>0.614797</td>\n",
       "      <td>0.696751</td>\n",
       "      <td>0.737556</td>\n",
       "      <td>0.794730</td>\n",
       "      <td>0.796240</td>\n",
       "      <td>0.833270</td>\n",
       "      <td>0.823509</td>\n",
       "      <td>0.836787</td>\n",
       "      <td>0.837526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809996</th>\n",
       "      <td>0.305650</td>\n",
       "      <td>0.295621</td>\n",
       "      <td>0.256275</td>\n",
       "      <td>0.220107</td>\n",
       "      <td>0.164272</td>\n",
       "      <td>0.116024</td>\n",
       "      <td>0.078614</td>\n",
       "      <td>0.041309</td>\n",
       "      <td>0.043083</td>\n",
       "      <td>0.037534</td>\n",
       "      <td>...</td>\n",
       "      <td>0.597710</td>\n",
       "      <td>0.678775</td>\n",
       "      <td>0.699154</td>\n",
       "      <td>0.747222</td>\n",
       "      <td>0.750222</td>\n",
       "      <td>0.778416</td>\n",
       "      <td>0.775639</td>\n",
       "      <td>0.751201</td>\n",
       "      <td>0.733458</td>\n",
       "      <td>0.702266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809997</th>\n",
       "      <td>0.256986</td>\n",
       "      <td>0.236297</td>\n",
       "      <td>0.185514</td>\n",
       "      <td>0.127390</td>\n",
       "      <td>0.098014</td>\n",
       "      <td>0.038709</td>\n",
       "      <td>0.032116</td>\n",
       "      <td>0.014756</td>\n",
       "      <td>0.064815</td>\n",
       "      <td>0.103352</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665234</td>\n",
       "      <td>0.699309</td>\n",
       "      <td>0.700876</td>\n",
       "      <td>0.711074</td>\n",
       "      <td>0.722581</td>\n",
       "      <td>0.695186</td>\n",
       "      <td>0.656056</td>\n",
       "      <td>0.610674</td>\n",
       "      <td>0.509762</td>\n",
       "      <td>0.383782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809998</th>\n",
       "      <td>0.213730</td>\n",
       "      <td>0.177011</td>\n",
       "      <td>0.125791</td>\n",
       "      <td>0.080640</td>\n",
       "      <td>0.036302</td>\n",
       "      <td>0.007403</td>\n",
       "      <td>0.035937</td>\n",
       "      <td>0.043684</td>\n",
       "      <td>0.110988</td>\n",
       "      <td>0.178333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.674421</td>\n",
       "      <td>0.688302</td>\n",
       "      <td>0.676385</td>\n",
       "      <td>0.636548</td>\n",
       "      <td>0.585522</td>\n",
       "      <td>0.532202</td>\n",
       "      <td>0.401940</td>\n",
       "      <td>0.259657</td>\n",
       "      <td>0.153737</td>\n",
       "      <td>0.129502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809999</th>\n",
       "      <td>0.145271</td>\n",
       "      <td>0.094687</td>\n",
       "      <td>0.068512</td>\n",
       "      <td>0.025310</td>\n",
       "      <td>0.002189</td>\n",
       "      <td>0.030263</td>\n",
       "      <td>0.057063</td>\n",
       "      <td>0.123217</td>\n",
       "      <td>0.214317</td>\n",
       "      <td>0.284864</td>\n",
       "      <td>...</td>\n",
       "      <td>0.665149</td>\n",
       "      <td>0.615267</td>\n",
       "      <td>0.562664</td>\n",
       "      <td>0.501671</td>\n",
       "      <td>0.392020</td>\n",
       "      <td>0.282861</td>\n",
       "      <td>0.181269</td>\n",
       "      <td>0.223284</td>\n",
       "      <td>0.350320</td>\n",
       "      <td>0.500050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>810000 rows × 226 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0         1         2         3         4         5         6  \\\n",
       "0       0.254551  0.258823  0.254659  0.252085  0.247678  0.253614  0.246511   \n",
       "1       0.205062  0.225544  0.217758  0.202169  0.199633  0.207380  0.191318   \n",
       "2       0.189196  0.165869  0.177655  0.156822  0.175094  0.177755  0.157582   \n",
       "3       0.131003  0.120076  0.138975  0.117931  0.130566  0.131262  0.126962   \n",
       "4       0.091033  0.086893  0.108125  0.080405  0.105917  0.077083  0.097895   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "809995  0.349513  0.342661  0.326351  0.274503  0.253076  0.200509  0.159043   \n",
       "809996  0.305650  0.295621  0.256275  0.220107  0.164272  0.116024  0.078614   \n",
       "809997  0.256986  0.236297  0.185514  0.127390  0.098014  0.038709  0.032116   \n",
       "809998  0.213730  0.177011  0.125791  0.080640  0.036302  0.007403  0.035937   \n",
       "809999  0.145271  0.094687  0.068512  0.025310  0.002189  0.030263  0.057063   \n",
       "\n",
       "               7         8         9  ...       216       217       218  \\\n",
       "0       0.259407  0.260862  0.242524  ...  0.354750  0.369223  0.388184   \n",
       "1       0.195369  0.200536  0.197588  ...  0.557203  0.573656  0.587998   \n",
       "2       0.158885  0.156911  0.166162  ...  0.699864  0.708688  0.721982   \n",
       "3       0.134453  0.106717  0.127309  ...  0.764786  0.763788  0.770017   \n",
       "4       0.086765  0.078676  0.075729  ...  0.786677  0.802271  0.806557   \n",
       "...          ...       ...       ...  ...       ...       ...       ...   \n",
       "809995  0.094644  0.074915  0.054888  ...  0.534272  0.614797  0.696751   \n",
       "809996  0.041309  0.043083  0.037534  ...  0.597710  0.678775  0.699154   \n",
       "809997  0.014756  0.064815  0.103352  ...  0.665234  0.699309  0.700876   \n",
       "809998  0.043684  0.110988  0.178333  ...  0.674421  0.688302  0.676385   \n",
       "809999  0.123217  0.214317  0.284864  ...  0.665149  0.615267  0.562664   \n",
       "\n",
       "             219       220       221       222       223       224       225  \n",
       "0       0.408496  0.414564  0.429403  0.419225  0.443250  0.433414  0.465502  \n",
       "1       0.612754  0.627825  0.633393  0.637706  0.625981  0.653231  0.637853  \n",
       "2       0.713464  0.743030  0.741709  0.747743  0.746037  0.737356  0.750391  \n",
       "3       0.787571  0.778866  0.776969  0.774712  0.801526  0.805305  0.784057  \n",
       "4       0.799614  0.789333  0.804087  0.787763  0.794948  0.819105  0.801781  \n",
       "...          ...       ...       ...       ...       ...       ...       ...  \n",
       "809995  0.737556  0.794730  0.796240  0.833270  0.823509  0.836787  0.837526  \n",
       "809996  0.747222  0.750222  0.778416  0.775639  0.751201  0.733458  0.702266  \n",
       "809997  0.711074  0.722581  0.695186  0.656056  0.610674  0.509762  0.383782  \n",
       "809998  0.636548  0.585522  0.532202  0.401940  0.259657  0.153737  0.129502  \n",
       "809999  0.501671  0.392020  0.282861  0.181269  0.223284  0.350320  0.500050  \n",
       "\n",
       "[810000 rows x 226 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### 케라스를 통해 모델 생성을 시작합니다.\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Activation\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau,ModelCheckpoint,EarlyStopping\n",
    "\n",
    "# 콜백 정의\n",
    "callback_list =[\n",
    "    ReduceLROnPlateau(\n",
    "        monitor='val_loss',  # 검증 손실을 기준으로 callback이 호출\n",
    "        factor=0.2,          # callback 호출시 학습률을 1/2로 줄인다.\n",
    "        patience=5,         # epoch 5 동안 개선되지 않으면 실행\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        filepath='my_model.{epoch:02d}-{val_loss:.4f}.hdf5',  #저장\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,    #가장 좋은 모델\n",
    "    ),\n",
    "    EarlyStopping(\n",
    "        monitor='val_loss',  #epochs 10번동안 val_loss값이 개선되지 않으면 학습종료(과대적합 방지)\n",
    "        patience=10,\n",
    "    )\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 512)               116224    \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 1024)              4096      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 512)               2048      \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 256)               1024      \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 1,605,508\n",
      "Trainable params: 1,599,620\n",
      "Non-trainable params: 5,888\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# mlp 모델 생성\n",
    "# 은닉층의 depth와 유닛 node수를 적절히 조정하여 최적의 모델을 찾아야함.\n",
    "# 학습 효율을 높이기 위해 배치 정규화 - dropout 대체\n",
    "# 오차 역전파 과정에서 미분한 gradient가 지나치게 커지거나 소실되는 문제를 해결하기위해 가중치 초기화 함수 사용 - he_normal\n",
    "# 각 층의 활성화 함수의 출력값 분포가 골고루 분포되도록 batch normalization 사용\n",
    "# ELU는 ReLU의 특성을 공유하고, dead ReLU의 문제점을 해결할 수 있는 활성함수이다.\n",
    "# 최적화 함수인 Optimaizer는 가장 많이 사용되고있는 adam을 사용하였다.\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(512, input_dim = 226, kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "# 활성함수 \n",
    "model.add(Activation('elu'))\n",
    "\n",
    "model.add(Dense(1024, kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "\n",
    "model.add(Dense(512,kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "\n",
    "model.add(Dense(512,kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "\n",
    "model.add(Dense(256,kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "\n",
    "model.add(Dense(128,kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Activation('elu'))\n",
    "\n",
    "model.add(Dense(units=4,kernel_initializer='he_normal', activation='linear'))\n",
    "          \n",
    "model.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 769500 samples, validate on 40500 samples\n",
      "Epoch 1/1\n",
      "625664/769500 [=======================>......] - ETA: 34s - loss: 60.5490 - mae: 60.5491"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "\n",
    "base_model = model.fit(train_X, train_Y, epochs=1, batch_size=256, validation_split = 0.05, callbacks=callback_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#예측값을 생성합니다.\n",
    "pred_test = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#submission 파일을 생성합니다.\n",
    "sample_sub = pd.read_csv('sample_submission.csv', index_col=0)\n",
    "submission = sample_sub+pred_test\n",
    "submission.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습한 모델을 저장. 필요에따라 load하여 학습을 이어갈 수 있음\n",
    "from keras.models import load_model\n",
    "model.save('semicon_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
